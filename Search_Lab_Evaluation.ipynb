{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd05dc6-487c-47a2-b373-9dd6ba184a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# End-to-End Vector Search Evaluation with Databricks & MLflow\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Vector Database Setup**: Create Delta Sync indexes that automatically generate embeddings from source columns\n",
    "2. **Retrieval Evaluation**: Compare hybrid vs dense retrieval using MLflow\n",
    "3. **Performance Analysis**: Analyze and visualize comparative results\n",
    "\n",
    "**Models Evaluated:**\n",
    "- `databricks-gte-large-en`\n",
    "- `databricks-bge-large-en`\n",
    "\n",
    "**Retrieval Types:**\n",
    "- Hybrid Search (combines keyword + semantic)\n",
    "- Dense Retrieval (semantic only)\n",
    "\n",
    "**Key Architecture:**\n",
    "- Delta table with CDC enabled\n",
    "- Delta Sync indexes with `embedding_source_column` + `embedding_model_endpoint_name`\n",
    "- Automatic embedding generation (no manual embedding tables!)\n",
    "- Hybrid search: Just pass `query_text` \n",
    "- Dense search: Generate query embedding + pass `query_vector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e8f86c-124d-432f-8148-1f05fb027137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b11360-4c3d-4b76-9c6a-d45c62ec78fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"datasets<4.0.0\" databricks-vectorsearch mlflow pandas numpy matplotlib seaborn --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51345ad3-60f4-494e-9099-7944a082763a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Databricks imports\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, monotonically_increasing_id\n",
    "\n",
    "# MLflow imports\n",
    "import mlflow\n",
    "import mlflow.data\n",
    "from mlflow.metrics.genai import relevance, answer_similarity\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize clients\n",
    "w = WorkspaceClient()\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e41c5fe-f097-45d6-bc2f-e8f757344ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9418e18e-5e50-41e8-b9d6-f4ee28f99d13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Configuration Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# Catalog and schema configuration\n",
    "CATALOG = \"main\"  # Update with your catalog\n",
    "SCHEMA = \"default\"  # Update with your schema\n",
    "TABLE_NAME = \"wikipedia_docs\"\n",
    "VECTOR_SEARCH_ENDPOINT = \"vector_search_endpoint\"  # Update with your endpoint name\n",
    "\n",
    "# Embedding models to compare\n",
    "EMBEDDING_MODELS = [\n",
    "    \"databricks-gte-large-en\",\n",
    "    \"databricks-bge-large-en\"\n",
    "]\n",
    "\n",
    "# MLflow experiment\n",
    "EXPERIMENT_NAME = \"/Users/{}/vector_search_evaluation_new\".format(\n",
    "    spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    ")\n",
    "\n",
    "# Sample size for evaluation\n",
    "SAMPLE_SIZE = 1000\n",
    "EVAL_SAMPLE_SIZE = 50  # Number of queries to evaluate\n",
    "\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Schema: {SCHEMA}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6a498f0-b725-4929-95d4-9e6d580a7ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Vector Database Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8153f0d3-996f-47a2-951f-ee60727dee00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1 Load Dataset\n",
    "We'll use a subset of Wikipedia articles as our document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4d93c0-9f9b-498a-a112-b4cb9d210f90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Wikipedia Dataset"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample Wikipedia dataset\n",
    "print(\"Loading Wikipedia dataset...\")\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True, cache_dir=\"/tmp/hf_cache2\", trust_remote_code=True)\n",
    "\n",
    "# Take first 1000 documents\n",
    "documents = []\n",
    "for i, doc in enumerate(dataset):\n",
    "    if i >= SAMPLE_SIZE:\n",
    "        break\n",
    "    documents.append({\n",
    "        \"id\": str(i),\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"text\": doc[\"text\"][:1000],  # Truncate to first 1000 chars\n",
    "        \"url\": doc[\"url\"]\n",
    "    })\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Loaded {i + 1} documents...\")\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Display sample\n",
    "df_docs = pd.DataFrame(documents)\n",
    "display(df_docs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8cd6d4e-9d92-46b7-aa79-17a9d3741bac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Spark DataFrame and Delta Table with CDC"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_docs)\n",
    "\n",
    "# Add unique identifier using monotonically_increasing_id\n",
    "spark_df = spark_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Create Delta table with Change Data Feed enabled (required for Delta Sync)\n",
    "full_table_name = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "# Drop table if exists (for demo purposes)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "\n",
    "# Write to Delta with CDC enabled\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(full_table_name)\n",
    "\n",
    "print(f\"✓ Created Delta table: {full_table_name}\")\n",
    "print(f\"  Total documents: {spark.table(full_table_name).count()}\")\n",
    "print(f\"  Change Data Feed: ENABLED\")\n",
    "\n",
    "# Display sample\n",
    "display(spark.table(full_table_name).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f92b62-8a0c-4436-b439-4653106bf952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2 Create Vector Search Endpoint\n",
    "A Vector Search endpoint is required to host the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd07b4e7-7798-401f-8115-3d85525c7541",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create or Get Vector Search Endpoint"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to get existing endpoint\n",
    "    endpoint = vsc.get_endpoint(VECTOR_SEARCH_ENDPOINT)\n",
    "    print(f\"✓ Using existing endpoint: {VECTOR_SEARCH_ENDPOINT}\")\n",
    "except Exception as e:\n",
    "    print(f\"Creating new endpoint: {VECTOR_SEARCH_ENDPOINT}\")\n",
    "    vsc.create_endpoint(\n",
    "        name=VECTOR_SEARCH_ENDPOINT,\n",
    "        endpoint_type=\"STANDARD\"\n",
    "    )\n",
    "    print(f\"✓ Created endpoint: {VECTOR_SEARCH_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7109e4a6-d2fc-4ae1-93f9-3a70d49996c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.3 Create Vector Search Indexes with Automatic Embeddings\n",
    "\n",
    "Delta Sync indexes automatically generate embeddings using the specified model endpoint.\n",
    "No need to manually create embedding tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d4024c-b421-4b75-8849-ac9bb9576054",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Vector Search Indexes with Delta Sync"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "indexes_info = {}\n",
    "\n",
    "for model_name in EMBEDDING_MODELS:\n",
    "    index_name = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}_{model_name.replace('-', '_')}_index2\"\n",
    "    \n",
    "    print(f\"\\nProcessing index for {model_name}...\")\n",
    "    print(f\"  Index name: {index_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if index already exists\n",
    "        try:\n",
    "            existing_index = vsc.get_index(index_name=index_name)\n",
    "            index_status = existing_index.describe()\n",
    "            state = index_status.get(\"status\", {}).get(\"state\", \"Ready\")\n",
    "            \n",
    "            print(f\"  ✓ Index already exists (state: {state})\")\n",
    "            \n",
    "            # Add to indexes_info without recreating\n",
    "            indexes_info[model_name] = {\n",
    "                \"index_name\": index_name,\n",
    "                \"model\": model_name,\n",
    "                \"source_table\": full_table_name\n",
    "            }\n",
    "            continue  # Skip to next model\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Index doesn't exist, proceed with creation\n",
    "            if \"NOT_FOUND\" in str(e) or \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "                print(f\"  Index not found, creating new index...\")\n",
    "            else:\n",
    "                # Some other error, re-raise\n",
    "                raise\n",
    "        \n",
    "        # Create Delta Sync index with automatic embedding generation\n",
    "        print(f\"  Source table: {full_table_name}\")\n",
    "        print(f\"  Embedding source: text column\")\n",
    "        \n",
    "        index = vsc.create_delta_sync_index(\n",
    "            endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "            source_table_name=full_table_name,\n",
    "            index_name=index_name,\n",
    "            pipeline_type=\"TRIGGERED\",\n",
    "            primary_key=\"id\",\n",
    "            embedding_source_column=\"text\",\n",
    "            embedding_model_endpoint_name=model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Created index: {index_name}\")\n",
    "        print(f\"  ✓ Embeddings will be auto-generated from 'text' column\")\n",
    "        print(f\"  ✓ Using model: {model_name}\")\n",
    "        \n",
    "        indexes_info[model_name] = {\n",
    "            \"index_name\": index_name,\n",
    "            \"model\": model_name,\n",
    "            \"source_table\": full_table_name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\n✓ Processed {len(indexes_info)} vector search indexes\")\n",
    "print(\"\\nNote: New indexes are syncing and generating embeddings in the background...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b054396a-9aad-4d29-8e8c-66a110160a95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wait for Index Sync to Complete"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Waiting for indexes to sync and generate embeddings...\")\n",
    "print(\"(This may take several minutes for 1K documents)\\n\")\n",
    "\n",
    "for model_name, info in indexes_info.items():\n",
    "    index = vsc.get_index(index_name=info[\"index_name\"])\n",
    "    \n",
    "    # Wait for index to be ready\n",
    "    max_wait = 600  # 10 minutes (embedding generation takes time)\n",
    "    wait_time = 0\n",
    "    last_status = None\n",
    "    \n",
    "    while wait_time < max_wait:\n",
    "        try:\n",
    "            status = index.describe()\n",
    "            index_status = status.get(\"status\", {})\n",
    "            state = index_status.get(\"ready\", False)\n",
    "            \n",
    "            if state:\n",
    "                row_count = index_status.get(\"indexed_row_count\", 0)\n",
    "                print(f\"✓ {model_name}: READY ({row_count} documents indexed)\")\n",
    "                break\n",
    "            \n",
    "            current_status = index_status.get(\"message\", \"Syncing...\")\n",
    "            if current_status != last_status:\n",
    "                print(f\"  {model_name}: {current_status}\")\n",
    "                last_status = current_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {model_name}: Checking status... ({wait_time}s)\")\n",
    "        \n",
    "        time.sleep(15)\n",
    "        wait_time += 15\n",
    "    \n",
    "    if wait_time >= max_wait:\n",
    "        print(f\"⚠ {model_name}: Sync timeout - but continuing (check index status manually)\")\n",
    "\n",
    "print(\"\\n✓ All indexes ready for querying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36b7942-26b8-4b80-994f-c7ec007b51db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2: Define Retrieval Functions & Evaluate with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcdc72f6-9d01-47e1-953b-b938d8747028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Create Evaluation Dataset\n",
    "Generate queries based on document titles and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20347fe2-2096-4d3c-86f7-879639e76086",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Evaluation Queries"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Sample documents for evaluation\n",
    "eval_indices = np.random.choice(len(documents), EVAL_SAMPLE_SIZE, replace=False)\n",
    "eval_docs = [documents[i] for i in eval_indices]\n",
    "\n",
    "# Generate queries (using title + first sentence of text)\n",
    "eval_data = []\n",
    "for doc in eval_docs:\n",
    "    # Create a question based on the document\n",
    "    first_sentence = doc[\"text\"].split(\".\")[0] if \".\" in doc[\"text\"] else doc[\"text\"][:100]\n",
    "    query = f\"What is information about {doc['title']}?\"\n",
    "    \n",
    "    eval_data.append({\n",
    "        \"query\": query,\n",
    "        \"ground_truth_doc_id\": doc[\"id\"],\n",
    "        \"ground_truth_title\": doc[\"title\"],\n",
    "        \"ground_truth_text\": doc[\"text\"]\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(f\"✓ Generated {len(eval_df)} evaluation queries\")\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d13b953-99e3-42ea-a34d-dc712f09b440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Define Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441a74b0-311c-4c84-bd6f-3938fe788f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Testing retrieval\n",
    "results = index.similarity_search(\n",
    "            columns=[\"id\", \"title\", \"text\", \"url\"],\n",
    "            query_text=\"What is information about Astrology?\",  # Triggers: embedding generation + BM25 + fusion\n",
    "            num_results=5,\n",
    "            disable_notice=True\n",
    "        )\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08ef90f-acf4-4223-a4eb-356d8e046d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3 Run Evaluation with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3db67b1-9f85-4d13-849f-150edaed224a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from mlflow.genai import scorer\n",
    "from mlflow.entities import Feedback\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Prepare data\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df[\"ground_truth\"] = eval_df[\"ground_truth_doc_id\"].apply(lambda x: [f\"{float(x):.1f}\"])\n",
    "\n",
    "# Define custom retrieval scorers (without @scorer decorator for the base functions)\n",
    "def recall_at_k(outputs: List[str], expectations: dict, k: int = 5) -> Feedback:\n",
    "    \"\"\"Calculate recall@k for retrieval results\"\"\"\n",
    "    ground_truth = set(expectations.get(\"expected_response\", []))\n",
    "    retrieved = set(outputs[:k])\n",
    "    \n",
    "    if len(ground_truth) == 0:\n",
    "        return Feedback(value=0.0, rationale=\"No ground truth provided\")\n",
    "    \n",
    "    hits = len(ground_truth & retrieved)\n",
    "    recall = hits / len(ground_truth)\n",
    "    \n",
    "    return Feedback(\n",
    "        value=recall,\n",
    "        rationale=f\"Retrieved {hits}/{len(ground_truth)} relevant documents in top-{k}\"\n",
    "    )\n",
    "\n",
    "def precision_at_k(outputs: List[str], expectations: dict, k: int = 5) -> Feedback:\n",
    "    \"\"\"Calculate precision@k for retrieval results\"\"\"\n",
    "    ground_truth = set(expectations.get(\"expected_response\", []))\n",
    "    retrieved = outputs[:k]\n",
    "    \n",
    "    if len(retrieved) == 0:\n",
    "        return Feedback(value=0.0, rationale=\"No documents retrieved\")\n",
    "    \n",
    "    hits = len(set(retrieved) & ground_truth)\n",
    "    precision = hits / len(retrieved)\n",
    "    \n",
    "    return Feedback(\n",
    "        value=precision,\n",
    "        rationale=f\"Found {hits} relevant documents in {len(retrieved)} retrieved\"\n",
    "    )\n",
    "\n",
    "def ndcg_at_k(outputs: List[str], expectations: dict, k: int = 10) -> Feedback:\n",
    "    \"\"\"Calculate NDCG@k for retrieval results\"\"\"\n",
    "    ground_truth = set(expectations.get(\"expected_response\", []))\n",
    "    retrieved = outputs[:k]\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved):\n",
    "        if doc_id in ground_truth:\n",
    "            # Relevance is 1 if document is in ground truth\n",
    "            dcg += 1.0 / np.log2(i + 2)  # i+2 because position starts at 0\n",
    "    \n",
    "    # Calculate IDCG (perfect ranking)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(ground_truth), k)))\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return Feedback(value=0.0, rationale=\"No relevant documents in ground truth\")\n",
    "    \n",
    "    ndcg = dcg / idcg\n",
    "    return Feedback(\n",
    "        value=ndcg,\n",
    "        rationale=f\"NDCG@{k} = {ndcg:.4f} (DCG={dcg:.4f}, IDCG={idcg:.4f})\"\n",
    "    )\n",
    "\n",
    "# Create decorated scorer variants using wrapper functions\n",
    "@scorer\n",
    "def recall_1(outputs, expectations):\n",
    "    return recall_at_k(outputs, expectations, k=1)\n",
    "\n",
    "@scorer\n",
    "def recall_3(outputs, expectations):\n",
    "    return recall_at_k(outputs, expectations, k=3)\n",
    "\n",
    "@scorer\n",
    "def recall_5(outputs, expectations):\n",
    "    return recall_at_k(outputs, expectations, k=5)\n",
    "\n",
    "@scorer\n",
    "def recall_10(outputs, expectations):\n",
    "    return recall_at_k(outputs, expectations, k=10)\n",
    "\n",
    "@scorer\n",
    "def precision_5(outputs, expectations):\n",
    "    return precision_at_k(outputs, expectations, k=5)\n",
    "\n",
    "@scorer\n",
    "def ndcg_10(outputs, expectations):\n",
    "    return ndcg_at_k(outputs, expectations, k=10)\n",
    "\n",
    "def retrieve_documents(query: str, index_name: str, query_type: str = \"hybrid\", k: int = 10) -> List[str]:\n",
    "    \"\"\"Retrieve documents from vector index\"\"\"\n",
    "    index = vsc.get_index(index_name=index_name)\n",
    "    results = index.similarity_search(\n",
    "        columns=[\"id\"],\n",
    "        query_text=query,\n",
    "        num_results=k,\n",
    "        query_type=query_type,\n",
    "        disable_notice=True\n",
    "    )\n",
    "    data_array = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "    doc_ids = [f\"{float(row[0]):.1f}\" for row in data_array]\n",
    "    return doc_ids\n",
    "\n",
    "# Compare both models\n",
    "model_comparison = {}\n",
    "all_eval_results = {}  # Store detailed results for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e95061-65aa-4c8c-9e2e-ba3339d7555b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for query_type in [\"FULL_TEXT\", \"hybrid\", \"ANN\"]:\n",
    "    for model_name, info in indexes_info.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING: {model_name} - {query_type}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Prepare evaluation data with retrieved documents\n",
    "        eval_data_with_outputs = []\n",
    "        for row in eval_data:\n",
    "            retrieved_docs = retrieve_documents(\n",
    "                row[\"query\"], \n",
    "                info['index_name'], \n",
    "                query_type, \n",
    "                10\n",
    "            )\n",
    "            eval_data_with_outputs.append({\n",
    "                \"inputs\": {\"query\": row[\"query\"]},\n",
    "                \"outputs\": retrieved_docs,\n",
    "                \"expectations\": {\n",
    "                    \"expected_response\": [f\"{float(row['ground_truth_doc_id']):.1f}\"]\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"retrieval_{model_name}_{query_type}\"):\n",
    "            mlflow.log_param(\"model\", model_name)\n",
    "            mlflow.log_param(\"index\", info['index_name'])\n",
    "            mlflow.log_param(\"query_type\", query_type)\n",
    "            \n",
    "            # Run evaluation with custom scorers\n",
    "            results = mlflow.genai.evaluate(\n",
    "                data=eval_data_with_outputs,\n",
    "                scorers=[recall_1, recall_3, recall_5, recall_10, precision_5, ndcg_10]\n",
    "            )\n",
    "            \n",
    "            # Store detailed results for analysis\n",
    "            run_key = f\"{model_name}_{query_type}\"\n",
    "            all_eval_results[run_key] = results\n",
    "            \n",
    "            # Extract aggregate metrics\n",
    "            model_comparison[run_key] = {\n",
    "                \"recall@1\": results.metrics[\"recall_1/mean\"],\n",
    "                \"recall@3\": results.metrics[\"recall_3/mean\"],\n",
    "                \"recall@5\": results.metrics[\"recall_5/mean\"],\n",
    "                \"recall@10\": results.metrics[\"recall_10/mean\"],\n",
    "                \"precision@5\": results.metrics[\"precision_5/mean\"],\n",
    "                \"ndcg@10\": results.metrics[\"ndcg_10/mean\"]\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nResults:\")\n",
    "            for k, v in model_comparison[run_key].items():\n",
    "                print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "comparison_df = pd.DataFrame(model_comparison).T\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nWinner by metric:\")\n",
    "for metric in comparison_df.columns:\n",
    "    winner = comparison_df[metric].idxmax()\n",
    "    print(f\"  {metric}: {winner}\")\n",
    "    \n",
    "print(\"\\n✓ Evaluation complete! Results stored in 'all_eval_results' for downstream analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "analysis-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3 Analyze Individual Examples\n",
    "\n",
    "Examine which specific queries caused performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7261d1f9-84bc-4f70-a8d0-82d05e7bc15a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764915287202}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all runs from the experiment\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"start_time DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"✓ Retrieved {len(runs_df)} runs from MLflow experiment\")\n",
    "display(runs_df[[\"run_id\", \"tags.mlflow.runName\", \"params.model\", \"params.query_type\", \n",
    "                 \"metrics.recall_1/mean\", \"metrics.recall_5/mean\", \"metrics.ndcg_10/mean\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81415cd6-561f-4e8e-bf83-78545df6d225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    run_id= \"6fec0c66fbbd49899d5acb821e8037e5\"\n",
    ")\n",
    "display(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2813d623-1b72-46b3-a99c-19dc32c19e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for traces where recall_1 < 1.0\n",
    "bad_recall_1 = traces[\n",
    "    traces['assessments'].apply(\n",
    "        lambda assessments: any(\n",
    "            a['assessment_name'] == 'recall_1' and a.get('feedback', {}).get('value', 1) < 1.0\n",
    "            for a in assessments\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "display(bad_recall_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaec7aa5-f664-4d61-a821-6e14815a2fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Testing retrieval\n",
    "index = vsc.get_index(index_name=\"main.default.wikipedia_docs_databricks_bge_large_en_index2\")\n",
    "results = index.similarity_search(\n",
    "            columns=[\"id\", \"title\", \"text\", \"url\"],\n",
    "            query_text=\"What is information about Artificial intelligence?\", \n",
    "            num_results=5,\n",
    "            disable_notice=True,\n",
    "            query_type=\"hybrid\"\n",
    "        )\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ZocDoc_Search_Lab_Modified",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
